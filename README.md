### Local Perplexity





## ğŸ“ Sobre o Projeto

O **Local Perplexity** Ã© uma aplicaÃ§Ã£o de pesquisa avanÃ§ada que combina modelos de linguagem locais (LLMs) com pesquisa web em tempo real para responder perguntas dos usuÃ¡rios de maneira detalhada e fundamentada. Inspirado no serviÃ§o Perplexity.ai, este projeto permite executar pesquisas robustas usando modelos executados localmente, proporcionando respostas tÃ©cnicas precisas com citaÃ§Ãµes adequadas.

## ğŸ—ï¸ Arquitetura

O Local Perplexity utiliza uma arquitetura baseada em grafos de estados para orquestrar o fluxo de trabalho de pesquisa e sÃ­ntese de informaÃ§Ãµes:

```mermaid
Arquitetura do Local Perplexity.download-icon {
            cursor: pointer;
            transform-origin: center;
        }
        .download-icon .arrow-part {
            transition: transform 0.35s cubic-bezier(0.35, 0.2, 0.14, 0.95);
             transform-origin: center;
        }
        button:has(.download-icon):hover .download-icon .arrow-part, button:has(.download-icon):focus-visible .download-icon .arrow-part {
          transform: translateY(-1.5px);
        }
        #mermaid-diagram-r107{font-family:var(--font-geist-sans);font-size:12px;fill:#000000;}#mermaid-diagram-r107 .error-icon{fill:#552222;}#mermaid-diagram-r107 .error-text{fill:#552222;stroke:#552222;}#mermaid-diagram-r107 .edge-thickness-normal{stroke-width:1px;}#mermaid-diagram-r107 .edge-thickness-thick{stroke-width:3.5px;}#mermaid-diagram-r107 .edge-pattern-solid{stroke-dasharray:0;}#mermaid-diagram-r107 .edge-thickness-invisible{stroke-width:0;fill:none;}#mermaid-diagram-r107 .edge-pattern-dashed{stroke-dasharray:3;}#mermaid-diagram-r107 .edge-pattern-dotted{stroke-dasharray:2;}#mermaid-diagram-r107 .marker{fill:#666;stroke:#666;}#mermaid-diagram-r107 .marker.cross{stroke:#666;}#mermaid-diagram-r107 svg{font-family:var(--font-geist-sans);font-size:12px;}#mermaid-diagram-r107 p{margin:0;}#mermaid-diagram-r107 .label{font-family:var(--font-geist-sans);color:#000000;}#mermaid-diagram-r107 .cluster-label text{fill:#333;}#mermaid-diagram-r107 .cluster-label span{color:#333;}#mermaid-diagram-r107 .cluster-label span p{background-color:transparent;}#mermaid-diagram-r107 .label text,#mermaid-diagram-r107 span{fill:#000000;color:#000000;}#mermaid-diagram-r107 .node rect,#mermaid-diagram-r107 .node circle,#mermaid-diagram-r107 .node ellipse,#mermaid-diagram-r107 .node polygon,#mermaid-diagram-r107 .node path{fill:#eee;stroke:#999;stroke-width:1px;}#mermaid-diagram-r107 .rough-node .label text,#mermaid-diagram-r107 .node .label text{text-anchor:middle;}#mermaid-diagram-r107 .node .katex path{fill:#000;stroke:#000;stroke-width:1px;}#mermaid-diagram-r107 .node .label{text-align:center;}#mermaid-diagram-r107 .node.clickable{cursor:pointer;}#mermaid-diagram-r107 .arrowheadPath{fill:#333333;}#mermaid-diagram-r107 .edgePath .path{stroke:#666;stroke-width:2.0px;}#mermaid-diagram-r107 .flowchart-link{stroke:#666;fill:none;}#mermaid-diagram-r107 .edgeLabel{background-color:white;text-align:center;}#mermaid-diagram-r107 .edgeLabel p{background-color:white;}#mermaid-diagram-r107 .edgeLabel rect{opacity:0.5;background-color:white;fill:white;}#mermaid-diagram-r107 .labelBkg{background-color:rgba(255, 255, 255, 0.5);}#mermaid-diagram-r107 .cluster rect{fill:hsl(0, 0%, 98.9215686275%);stroke:#707070;stroke-width:1px;}#mermaid-diagram-r107 .cluster text{fill:#333;}#mermaid-diagram-r107 .cluster span{color:#333;}#mermaid-diagram-r107 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:var(--font-geist-sans);font-size:12px;background:hsl(-160, 0%, 93.3333333333%);border:1px solid #707070;border-radius:2px;pointer-events:none;z-index:100;}#mermaid-diagram-r107 .flowchartTitleText{text-anchor:middle;font-size:18px;fill:#000000;}#mermaid-diagram-r107 .flowchart-link{stroke:hsl(var(--gray-400));stroke-width:1px;}#mermaid-diagram-r107 .marker,#mermaid-diagram-r107 marker,#mermaid-diagram-r107 marker *{fill:hsl(var(--gray-400))!important;stroke:hsl(var(--gray-400))!important;}#mermaid-diagram-r107 .label,#mermaid-diagram-r107 text,#mermaid-diagram-r107 text>tspan{fill:hsl(var(--black))!important;color:hsl(var(--black))!important;}#mermaid-diagram-r107 .background,#mermaid-diagram-r107 rect.relationshipLabelBox{fill:hsl(var(--white))!important;}#mermaid-diagram-r107 .entityBox,#mermaid-diagram-r107 .attributeBoxEven{fill:hsl(var(--gray-150))!important;}#mermaid-diagram-r107 .attributeBoxOdd{fill:hsl(var(--white))!important;}#mermaid-diagram-r107 .label-container,#mermaid-diagram-r107 rect.actor{fill:hsl(var(--white))!important;stroke:hsl(var(--gray-400))!important;}#mermaid-diagram-r107 line{stroke:hsl(var(--gray-400))!important;}#mermaid-diagram-r107 :root{--mermaid-font-family:var(--font-geist-sans);}ParaleloEntrada do UsuÃ¡riobuild_first_queriesspawn_researcherssingle_searchfinal_writerResposta Final
```

### Componentes Principais

1. **Interface do UsuÃ¡rio**: Implementada com Streamlit, fornece uma interface web intuitiva para inserÃ§Ã£o de perguntas.
2. **Motor de Grafo de Estados**: Utiliza LangGraph para orquestrar o fluxo de trabalho da aplicaÃ§Ã£o.
3. **Modelos de Linguagem**: Utiliza modelos Llama e DeepSeek atravÃ©s do Ollama para processamento de linguagem natural.
4. **Cliente de Pesquisa Web**: Integra-se com a API Tavily para realizar buscas na web e extrair conteÃºdo relevante.


## ğŸ”„ Fluxo de Dados

O fluxo de processamento do Local Perplexity segue estas etapas principais:

1. **Entrada do UsuÃ¡rio**: O usuÃ¡rio insere uma pergunta atravÃ©s da interface Streamlit.
2. **GeraÃ§Ã£o de Consultas**:
O sistema analisa a pergunta do usuÃ¡rio e gera mÃºltiplas consultas de pesquisa especÃ­ficas:

```python
def build_first_queries(state: ReportState):
    # Gera 3-5 consultas estratÃ©gicas baseadas na pergunta do usuÃ¡rio
    prompt = build_queries.format(user_input=user_input)
    query_llm = llm.with_structured_output(QueryList)
    result = query_llm.invoke(prompt)
    return {"queries": result.queries}
```


3. **Pesquisa Paralela**:
Cada consulta gerada Ã© processada paralelamente:

```python
def spawn_researchers(state: ReportState):
    return [Send("single_search", {"query": query, "user_input": state.user_input})
            for query in state.queries]
```


4. **Busca e SÃ­ntese Individual**:
Para cada consulta, o sistema:

1. Realiza uma busca web usando a API Tavily
2. Extrai o conteÃºdo das pÃ¡ginas encontradas
3. Sintetiza as informaÃ§Ãµes relevantes



5. **SÃ­ntese Final**:
Todas as informaÃ§Ãµes coletadas sÃ£o combinadas em uma resposta final coerente e bem estruturada, incluindo referÃªncias Ã s fontes.


## ğŸ§° Tecnologias Utilizadas

| Tecnologia | VersÃ£o | FunÃ§Ã£o
|-----|-----|-----
| **Streamlit** | 1.45.1 | Interface web interativa
| **LangGraph** | 0.4.8 | OrquestraÃ§Ã£o do fluxo de trabalho baseado em grafos
| **LangChain** | 0.3.25 | Framework para aplicaÃ§Ãµes com LLMs
| **Ollama** | 0.5.1 | ExecuÃ§Ã£o local de modelos de linguagem
| **Tavily Python** | 0.7.5 | Cliente para API de pesquisa web
| **Python-dotenv** | 1.1.0 | Gerenciamento de variÃ¡veis de ambiente
| **Pydantic** | 2.11.5 | ValidaÃ§Ã£o de esquemas de dados


## ğŸ“¦ InstalaÃ§Ã£o

### PrÃ©-requisitos

- Python 3.12+
- Ollama instalado e configurado com os modelos:

- `llama3.2:1b`
- `deepseek-r1:1.5b`



- Chave de API do Tavily para pesquisa web


### Passos para InstalaÃ§Ã£o

1. **Clone o repositÃ³rio**


```shellscript
git clone https://github.com/seu-usuario/local-perplexity.git
cd local-perplexity
```

2. **Configure o ambiente virtual (opcional, mas recomendado)**


```shellscript
python -m venv venv
source venv/bin/activate  # No Windows: venv\Scripts\activate
```

3. **Instale as dependÃªncias**


```shellscript
pip install -r requirements.txt
```

4. **Configure as variÃ¡veis de ambiente**


Crie um arquivo `.env` na raiz do projeto:

```plaintext
TAVILY_API_KEY=sua_chave_api_tavily
```

## ğŸš€ Como Usar

### Executando a AplicaÃ§Ã£o

```shellscript
python app.py
```

Isso iniciarÃ¡ o servidor Streamlit e abrirÃ¡ a aplicaÃ§Ã£o no navegador (geralmente em [http://localhost:8501](http://localhost:8501)).

### Interface da AplicaÃ§Ã£o

1. Digite sua pergunta no campo de texto.
2. Clique no botÃ£o "Pesquisar".
3. Acompanhe o progresso da pesquisa em tempo real.
4. Visualize a resposta gerada, incluindo as referÃªncias Ã s fontes utilizadas.
5. (Opcional) Expanda a seÃ§Ã£o "ğŸ§  ReflexÃ£o" para ver o raciocÃ­nio do modelo.


## ğŸ“ Estrutura do Projeto

```plaintext
local-perplexity/
â”œâ”€â”€ app.py                    # Ponto de entrada principal
â”œâ”€â”€ requirements.txt          # DependÃªncias do projeto
â”œâ”€â”€ pyproject.toml           # ConfiguraÃ§Ã£o do poetry
â”œâ”€â”€ poetry.lock              # Lock file das dependÃªncias
â””â”€â”€ src/
    â””â”€â”€ local_perplexity/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ graph.py         # DefiniÃ§Ã£o do grafo e interface
        â”œâ”€â”€ prompts.py       # Templates de prompts para os LLMs
        â”œâ”€â”€ schemas.py       # Esquemas de dados usando Pydantic
        â””â”€â”€ utils.py         # FunÃ§Ãµes utilitÃ¡rias e APIs
```

## ğŸ” Funcionalidades Principais

### GeraÃ§Ã£o Inteligente de Consultas

O sistema analisa a pergunta do usuÃ¡rio e gera automaticamente mÃºltiplas consultas especÃ­ficas para garantir uma cobertura abrangente do tÃ³pico.

### Pesquisa Web com Fontes VerificÃ¡veis

Utiliza a API Tavily para realizar buscas na web e extrair conteÃºdo completo de pÃ¡ginas relevantes, fornecendo informaÃ§Ãµes atualizadas e verificÃ¡veis.

### Processamento Paralelo

Executa mÃºltiplas pesquisas simultaneamente, melhorando a eficiÃªncia e a velocidade de resposta.

### SÃ­ntese AvanÃ§ada de InformaÃ§Ãµes

Combina informaÃ§Ãµes de mÃºltiplas fontes em uma resposta coerente, bem estruturada e tecnicamente precisa.

### Sistema de CitaÃ§Ãµes

Inclui referÃªncias numeradas para todas as fontes utilizadas, permitindo ao usuÃ¡rio verificar as informaÃ§Ãµes originais.

## ğŸ’» Exemplos de Prompts

Os prompts utilizados pelo sistema sÃ£o cuidadosamente estruturados para guiar os modelos de linguagem:

### Prompt para GeraÃ§Ã£o de Consultas

```python
build_queries = agent_prompt + """
Seu primeiro objetivo Ã© construir uma lista de consultas
que serÃ£o usadas para encontrar respostas Ã  pergunta do usuÃ¡rio.

Responda com algo entre 3-5 consultas.
"""
```

### Prompt para SÃ­ntese Final

```python
build_final_response = agent_prompt + """
Seu objetivo aqui Ã© desenvolver uma resposta final para o usuÃ¡rio usando
os relatÃ³rios feitos durante a pesquisa web, com suas sÃ­nteses.

A resposta deve conter algo entre 500 - 800 palavras.

Aqui estÃ£o os resultados da pesquisa web:
<SEARCH_RESULTS>
{search_results}
</SEARCH_RESULTS>

VocÃª deve adicionar citaÃ§Ãµes de referÃªncia (com o nÃºmero da citaÃ§Ã£o, exemplo: [1]) para os 
artigos que vocÃª usou em cada parÃ¡grafo da sua resposta.
"""
```

## âš ï¸ LimitaÃ§Ãµes Conhecidas

- Depende de modelos locais, exigindo recursos computacionais adequados
- Limitado a 1 resultado por consulta por padrÃ£o (configurÃ¡vel)
- Processamento sequencial de extraÃ§Ã£o de conteÃºdo pode ser lento para pÃ¡ginas complexas
- Qualidade das respostas depende dos modelos de linguagem utilizados


## ğŸ”® Melhorias Futuras

- ImplementaÃ§Ã£o de cache para consultas repetidas
- Suporte a mÃºltiplos provedores de pesquisa
- Interface mais rica com visualizaÃ§Ãµes e histÃ³rico de pesquisas
- Sistema de feedback do usuÃ¡rio para melhorar respostas futuras
- OpÃ§Ã£o para escolher diferentes modelos de linguagem
- OtimizaÃ§Ã£o de prompts para casos de uso especÃ­ficos


## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Sinta-se Ã  vontade para abrir issues ou pull requests.

1. FaÃ§a um fork do projeto
2. Crie sua branch de feature (`git checkout -b feature/nova-funcionalidade`)
3. Commit suas mudanÃ§as (`git commit -m 'Adiciona nova funcionalidade'`)
4. Push para a branch (`git push origin feature/nova-funcionalidade`)
5. Abra um Pull Request


## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a licenÃ§a MIT - veja o arquivo LICENSE para detalhes.

---

Desenvolvido por [Seu Nome] - [Seu Email]