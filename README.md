### Local Perplexity





## ğŸ“ Sobre o Projeto

O **Local Perplexity** Ã© uma aplicaÃ§Ã£o de pesquisa avanÃ§ada que combina modelos de linguagem locais (LLMs) com pesquisa web em tempo real para responder perguntas dos usuÃ¡rios de maneira detalhada e fundamentada. Inspirado no serviÃ§o Perplexity.ai, este projeto permite executar pesquisas robustas usando modelos executados localmente, proporcionando respostas tÃ©cnicas precisas com citaÃ§Ãµes adequadas.

## ğŸ—ï¸ Arquitetura

O Local Perplexity utiliza uma arquitetura baseada em grafos de estados para orquestrar o fluxo de trabalho de pesquisa e sÃ­ntese de informaÃ§Ãµes:

![Texto Alternativo](/home/kael/cursos/local_perplexity/local_perplexity.excalidraw.png)

### Componentes Principais

1. **Interface do UsuÃ¡rio**: Implementada com Streamlit, fornece uma interface web intuitiva para inserÃ§Ã£o de perguntas.
2. **Motor de Grafo de Estados**: Utiliza LangGraph para orquestrar o fluxo de trabalho da aplicaÃ§Ã£o.
3. **Modelos de Linguagem**: Utiliza modelos Llama e DeepSeek atravÃ©s do Ollama para processamento de linguagem natural.
4. **Cliente de Pesquisa Web**: Integra-se com a API Tavily para realizar buscas na web e extrair conteÃºdo relevante.


## ğŸ”„ Fluxo de Dados

O fluxo de processamento do Local Perplexity segue estas etapas principais:

1. **Entrada do UsuÃ¡rio**: O usuÃ¡rio insere uma pergunta atravÃ©s da interface Streamlit.
2. **GeraÃ§Ã£o de Consultas**:
O sistema analisa a pergunta do usuÃ¡rio e gera mÃºltiplas consultas de pesquisa especÃ­ficas:

```python
def build_first_queries(state: ReportState):
    # Gera 3-5 consultas estratÃ©gicas baseadas na pergunta do usuÃ¡rio
    prompt = build_queries.format(user_input=user_input)
    query_llm = llm.with_structured_output(QueryList)
    result = query_llm.invoke(prompt)
    return {"queries": result.queries}
```


3. **Pesquisa Paralela**:
Cada consulta gerada Ã© processada paralelamente:

```python
def spawn_researchers(state: ReportState):
    return [Send("single_search", {"query": query, "user_input": state.user_input})
            for query in state.queries]
```


4. **Busca e SÃ­ntese Individual**:
Para cada consulta, o sistema:

1. Realiza uma busca web usando a API Tavily
2. Extrai o conteÃºdo das pÃ¡ginas encontradas
3. Sintetiza as informaÃ§Ãµes relevantes



5. **SÃ­ntese Final**:
Todas as informaÃ§Ãµes coletadas sÃ£o combinadas em uma resposta final coerente e bem estruturada, incluindo referÃªncias Ã s fontes.


## ğŸ§° Tecnologias Utilizadas

| Tecnologia | VersÃ£o | FunÃ§Ã£o
|-----|-----|-----
| **Streamlit** | 1.45.1 | Interface web interativa
| **LangGraph** | 0.4.8 | OrquestraÃ§Ã£o do fluxo de trabalho baseado em grafos
| **LangChain** | 0.3.25 | Framework para aplicaÃ§Ãµes com LLMs
| **Ollama** | 0.5.1 | ExecuÃ§Ã£o local de modelos de linguagem
| **Tavily Python** | 0.7.5 | Cliente para API de pesquisa web
| **Python-dotenv** | 1.1.0 | Gerenciamento de variÃ¡veis de ambiente
| **Pydantic** | 2.11.5 | ValidaÃ§Ã£o de esquemas de dados


## ğŸ“¦ InstalaÃ§Ã£o

### PrÃ©-requisitos

- Python 3.12+
- Ollama instalado e configurado com os modelos:

- `llama3.2:1b`
- `deepseek-r1:1.5b`



- Chave de API do Tavily para pesquisa web


### Passos para InstalaÃ§Ã£o

1. **Clone o repositÃ³rio**


```shellscript
git clone https://github.com/seu-usuario/local-perplexity.git
cd local-perplexity
```

2. **Configure o ambiente virtual (opcional, mas recomendado)**


```shellscript
python -m venv venv
source venv/bin/activate  # No Windows: venv\Scripts\activate
```

3. **Instale as dependÃªncias**


```shellscript
pip install -r requirements.txt
```

4. **Configure as variÃ¡veis de ambiente**


Crie um arquivo `.env` na raiz do projeto:

```plaintext
TAVILY_API_KEY=sua_chave_api_tavily
```

## ğŸš€ Como Usar

### Executando a AplicaÃ§Ã£o

```shellscript
python app.py
```

Isso iniciarÃ¡ o servidor Streamlit e abrirÃ¡ a aplicaÃ§Ã£o no navegador (geralmente em [http://localhost:8501](http://localhost:8501)).

### Interface da AplicaÃ§Ã£o

1. Digite sua pergunta no campo de texto.
2. Clique no botÃ£o "Pesquisar".
3. Acompanhe o progresso da pesquisa em tempo real.
4. Visualize a resposta gerada, incluindo as referÃªncias Ã s fontes utilizadas.
5. (Opcional) Expanda a seÃ§Ã£o "ğŸ§  ReflexÃ£o" para ver o raciocÃ­nio do modelo.


## ğŸ“ Estrutura do Projeto

```plaintext
local-perplexity/
â”œâ”€â”€ app.py                    # Ponto de entrada principal
â”œâ”€â”€ requirements.txt          # DependÃªncias do projeto
â”œâ”€â”€ pyproject.toml           # ConfiguraÃ§Ã£o do poetry
â”œâ”€â”€ poetry.lock              # Lock file das dependÃªncias
â””â”€â”€ src/
    â””â”€â”€ local_perplexity/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ graph.py         # DefiniÃ§Ã£o do grafo e interface
        â”œâ”€â”€ prompts.py       # Templates de prompts para os LLMs
        â”œâ”€â”€ schemas.py       # Esquemas de dados usando Pydantic
        â””â”€â”€ utils.py         # FunÃ§Ãµes utilitÃ¡rias e APIs
```

## ğŸ” Funcionalidades Principais

### GeraÃ§Ã£o Inteligente de Consultas

O sistema analisa a pergunta do usuÃ¡rio e gera automaticamente mÃºltiplas consultas especÃ­ficas para garantir uma cobertura abrangente do tÃ³pico.

### Pesquisa Web com Fontes VerificÃ¡veis

Utiliza a API Tavily para realizar buscas na web e extrair conteÃºdo completo de pÃ¡ginas relevantes, fornecendo informaÃ§Ãµes atualizadas e verificÃ¡veis.

### Processamento Paralelo

Executa mÃºltiplas pesquisas simultaneamente, melhorando a eficiÃªncia e a velocidade de resposta.

### SÃ­ntese AvanÃ§ada de InformaÃ§Ãµes

Combina informaÃ§Ãµes de mÃºltiplas fontes em uma resposta coerente, bem estruturada e tecnicamente precisa.

### Sistema de CitaÃ§Ãµes

Inclui referÃªncias numeradas para todas as fontes utilizadas, permitindo ao usuÃ¡rio verificar as informaÃ§Ãµes originais.

## ğŸ’» Exemplos de Prompts

Os prompts utilizados pelo sistema sÃ£o cuidadosamente estruturados para guiar os modelos de linguagem:

### Prompt para GeraÃ§Ã£o de Consultas

```python
build_queries = agent_prompt + """
Seu primeiro objetivo Ã© construir uma lista de consultas
que serÃ£o usadas para encontrar respostas Ã  pergunta do usuÃ¡rio.

Responda com algo entre 3-5 consultas.
"""
```

### Prompt para SÃ­ntese Final

```python
build_final_response = agent_prompt + """
Seu objetivo aqui Ã© desenvolver uma resposta final para o usuÃ¡rio usando
os relatÃ³rios feitos durante a pesquisa web, com suas sÃ­nteses.

A resposta deve conter algo entre 500 - 800 palavras.

Aqui estÃ£o os resultados da pesquisa web:
<SEARCH_RESULTS>
{search_results}
</SEARCH_RESULTS>

VocÃª deve adicionar citaÃ§Ãµes de referÃªncia (com o nÃºmero da citaÃ§Ã£o, exemplo: [1]) para os 
artigos que vocÃª usou em cada parÃ¡grafo da sua resposta.
"""
```

## âš ï¸ LimitaÃ§Ãµes Conhecidas

- Depende de modelos locais, exigindo recursos computacionais adequados
- Limitado a 1 resultado por consulta por padrÃ£o (configurÃ¡vel)
- Processamento sequencial de extraÃ§Ã£o de conteÃºdo pode ser lento para pÃ¡ginas complexas
- Qualidade das respostas depende dos modelos de linguagem utilizados


## ğŸ”® Melhorias Futuras

- ImplementaÃ§Ã£o de cache para consultas repetidas
- Suporte a mÃºltiplos provedores de pesquisa
- Interface mais rica com visualizaÃ§Ãµes e histÃ³rico de pesquisas
- Sistema de feedback do usuÃ¡rio para melhorar respostas futuras
- OpÃ§Ã£o para escolher diferentes modelos de linguagem
- OtimizaÃ§Ã£o de prompts para casos de uso especÃ­ficos


## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Sinta-se Ã  vontade para abrir issues ou pull requests.

1. FaÃ§a um fork do projeto
2. Crie sua branch de feature (`git checkout -b feature/nova-funcionalidade`)
3. Commit suas mudanÃ§as (`git commit -m 'Adiciona nova funcionalidade'`)
4. Push para a branch (`git push origin feature/nova-funcionalidade`)
5. Abra um Pull Request


## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a licenÃ§a MIT - veja o arquivo LICENSE para detalhes.

---

Desenvolvido por [Seu Nome] - [Seu Email]